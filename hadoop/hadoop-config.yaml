apiVersion: v1
kind: ConfigMap
metadata:
  name: hadoop-config
  labels:
    app.kubernetes.io/name: hadoop
data:
  core-site.xml: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://gdh-cluster</value>
        </property>
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>Placeholder_zk_quorum</value>
        </property>
        <property>
            <name>hadoop.tmp.dir</name>
            <value>/data/dfs/tmp</value>
        </property>
        <property>
            <name>ipc.client.ping</name>
            <value>false</value>
        </property>
        <property>
            <name>ipc.ping.interval</name>
            <value>900000</value>
        </property>
        <property>
            <name>io.file.buffer.size</name>
            <value>131072</value>
        </property>
        <property>
            <name>hadoop.proxyuser.root.groups</name>
            <value>*</value>
        </property>
        <property>
            <name>hadoop.proxyuser.root.hosts</name>
            <value>*</value>
        </property>
    </configuration>
  hdfs-site.xml: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>dfs.nameservices</name>
            <value>gdh-cluster</value>
        </property>
        <property>
            <name>dfs.ha.namenodes.gdh-cluster</name>
            <value>nn1,nn2</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.gdh-cluster.nn1</name>
            <value>Placeholder_nn1_node:8020</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.gdh-cluster.nn2</name>
            <value>Placeholder_nn2_node:8020</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.gdh-cluster.nn1</name>
            <value>Placeholder_nn1_node:9870</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.gdh-cluster.nn2</name>
            <value>Placeholder_nn2_node:9870</value>
        </property>
        <property>
            <name>dfs.namenode.shared.edits.dir</name>
            <value>qjournal://Placeholder_qjournal/gdh-cluster</value>
        </property>
        <property>
            <name>dfs.journalnode.edits.dir</name>
            <value>/dfs/jn</value>
        </property>
        <property>
            <name>dfs.journalnode.http-address</name>
            <value>0.0.0.0:8480</value>
        </property>
        <property>
            <name>dfs.journalnode.rpc-address</name>
            <value>0.0.0.0:8485</value>
        </property>
        <property>
            <name>dfs.ha.automatic-failover.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>dfs.client.failover.proxy.provider.gdh-cluster</name>
            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <property>
            <name>dfs.ha.fencing.methods</name>
            <value>sshfence(root:9431)</value>
        </property>
        <property>
            <name>dfs.ha.fencing.ssh.private-key-files</name>
            <value>/root/.ssh/id_rsa</value>
        </property>
        <property>
            <name>dfs.ha.fencing.ssh.connect-timeout</name>
            <value>30000</value>
        </property>
        <property>
            <name>heartbeat.recheck.interval</name>
            <value>2000</value>
        </property>
        <property>
            <name>dfs.heartbeat.interval</name>
            <value>1</value>
        </property>
        <property>
            <name>dfs.blockreport.intervalMsec</name>
            <value>10000</value>
        </property>
        <property>
            <name>dfs.webhdfs.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>dfs.replication</name>
            <value>3</value>
        </property>
        <property>
            <name>dfs.blocksize</name>
            <value>134217728</value>
        </property>
        <property>
            <name>dfs.datanode.max.transfer.threads</name>
            <value>65536</value>
        </property>
        <property>
            <name>dfs.permissions</name>
            <value>true</value>
        </property>
        <property>
            <name>dfs.datanode.handler.count</name>
            <value>20</value>
        </property>
        <property>
            <name>dfs.namenode.name.dir</name>
            <value>/dfs/nn</value>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/dfs/dn</value>
        </property>
    </configuration>
  mapred-site.xml: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.address</name>
            <value>Placeholder_jobhistory:10020</value>
        </property>
        <property>
            <name>mapreduce.jobhistory.webapp.address</name>
            <value>Placeholder_jobhistory:19888</value>
        </property>
    </configuration>
  yarn-site.xml: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>yarn.resourcemanager.ha.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>yarn.resourcemanager.cluster-id</name>
            <value>yarn-gdh-cluster</value>
        </property>
        <property>
            <name>yarn.resourcemanager.ha.rm-ids</name>
            <value>rm1,rm2</value>
        </property>
        <property>
            <name>yarn.resourcemanager.hostname.rm1</name>
            <value>Placeholder_rm1_node</value>
        </property>
        <property>
            <name>yarn.resourcemanager.hostname.rm2</name>
            <value>Placeholder_rm2_node</value>
        </property>
        <property>
            <name>yarn.resourcemanager.zk-address</name>
            <value>Placeholder_zk_quorum</value>
        </property>
        <property>
            <name>yarn.resourcemanager.recovery.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>yarn.resourcemanager.store.class</name>
            <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
        </property>
        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>
        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>
        <property>
            <name>yarn.scheduler.minimum-allocation-mb</name>
            <value>1024</value>
            <describe>单个Container可调度最小内存</describe>
        </property>
        <property>
            <name>yarn.scheduler.maximum-allocation-mb</name>
            <value>16384</value>
            <describe>单个Container可调度最大内存</describe>
        </property>
        <property>
            <name>yarn.scheduler.minimum-allocation-vcores</name>
            <value>1</value>
            <describe>单个Container可调度最小cpu核数</describe>
        </property>
        <property>
            <name>yarn.scheduler.maximum-allocation-vcores</name>
            <value>4</value>
            <describe>单个Container可调度最大cpu核数</describe>
        </property>
        <property>
            <name>yarn.nodemanager.resource.memory-mb</name>
            <value>49152</value>
            <describe>节点内存大小,配合resource/limit定义</describe>
        </property>
        <property>
            <name>yarn.nodemanager.resource.cpu-vcores</name>
            <value>12</value>
            <describe>节点cpu大小,配合resource/limit定义</describe>
        </property>
        <property>
            <name>yarn.log-aggregation-enable</name>
            <value>true</value>
        </property>
        <property>
            <name>yarn.log.server.url</name>
            <value>http://Placeholder_jobhistory:19888/jobhistory/logs</value>
        </property>
        <property>
            <name>yarn.log-aggregation.retain-seconds</name>
            <value>604800</value>
        </property>
        <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
        </property>
        <property>
            <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
            <value>org.apache.hadoop.mapred.ShuffleHandler</value>
        </property>
        <property>
            <name>yarn.resourcemanager.bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>yarn.nodemanager.bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>yarn.timeline-service.bind-host</name>
            <value>0.0.0.0</value>
        </property>
        <property>
            <name>yarn.application.classpath</name>
            <value>
            /opt/hadoop/etc/hadoop,
            /opt/hadoop/share/hadoop/common/*,
            /opt/hadoop/share/hadoop/common/lib/*,
            /opt/hadoop/share/hadoop/hdfs/*,
            /opt/hadoop/share/hadoop/hdfs/lib/*,
            /opt/hadoop/share/hadoop/mapreduce/*,
            /opt/hadoop/share/hadoop/mapreduce/lib/*,
            /opt/hadoop/share/hadoop/yarn/*,
            /opt/hadoop/share/hadoop/yarn/lib/*
            </value>
        </property>
        <property>
            <name>yarn.resourcemanager.scheduler.class</name>
            <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
        </property>
        <property>
            <name>yarn.scheduler.fair.preemption</name>
            <value>true</value>
        </property>
        <property>
            <name>yarn.scheduler.fair.preemption.gdh-cluster-utilization-threshold</name>
            <value>1.0</value>
        </property>
    </configuration>
